{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaoloBarba/ADM_HW2-Group18/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algorithmic Methods of Data Mining**\n",
        "\n",
        "#Academic year 2022â€“2023\n",
        "\n",
        "# Homework 2 - Instagram Profiles & Posts\n",
        "\n",
        "Authors: Barba Paolo, Bellaroba Albachiara, Soukaina Alaoui."
      ],
      "metadata": {
        "id": "kYur1faNzt2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import the packages and upload datasets\n"
      ],
      "metadata": {
        "id": "z4aPRI3HA63g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wLM1xak2KwOU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm     \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "BGnoXZUTLWAw",
        "outputId": "d31cb5b2-ad79-45f0-cebc-32a15e58adab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-98be0bef3525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 125\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "locations =pd.read_csv('/content/drive/MyDrive/ADM_HW2/instagram_locations.csv',sep='\\t')\n",
        "profiles =pd.read_csv(\"/content/drive/MyDrive/ADM_HW2/instagram_profiles.csv\",sep='\\t')\n",
        "posts =pd.read_csv(\"/content/drive/MyDrive/ADM_HW2/instagram_posts.csv\",sep='\\t',nrows=1000000)"
      ],
      "metadata": {
        "id": "iX6V6SXCQ6tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RQ1 \n",
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "BpkqZqMECBDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform an explanatory data analysis, we will go through three different steps: data explorations, data manipulations, and data summary.\n",
        "First, we will show the first few rows of the datasets we are analyzing."
      ],
      "metadata": {
        "id": "VXsKZDOdCIRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "locations.head()"
      ],
      "metadata": {
        "id": "aVi6zjocbcK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profiles.head()"
      ],
      "metadata": {
        "id": "bj3WmAzyCMEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts.head()"
      ],
      "metadata": {
        "id": "c5dDdZl7CMMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now look at the columns of our datasets to understand which variables we are analyzing.\n",
        "First of all, we will do info method into the dataset to quickly understand the variables we are working with, get the number of rows and\n",
        "columns we have for each dataset."
      ],
      "metadata": {
        "id": "ChBPh66WZsXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "locations.info()"
      ],
      "metadata": {
        "id": "0mY-XE9VZWuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profiles.info()"
      ],
      "metadata": {
        "id": "K_wy7BA9ZnI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts.info()"
      ],
      "metadata": {
        "id": "WNvx6U-bZnK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Locations dataframe has \" + str(locations.shape[0]) + \" observations and \"  + str(locations.shape[1]) + \" variables\")\n",
        "print(\"Profiles dataframe has \" + str(profiles.shape[0]) + \" observations and \"  + str(profiles.shape[1]) + \" variables\")\n",
        "print(\"Posts dataframe has \" + str(posts.shape[0]) + \" observations and \"  + str(posts.shape[1]) + \" variables\")"
      ],
      "metadata": {
        "id": "O0skTFSVCa9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going one step further, having a better understanding of the statistical properties of these data frames by using a described method that gives us basic statistics of all the numerical columns we have. We are doing a statistical summarization at all the numeric variables that have the sense to summarize."
      ],
      "metadata": {
        "id": "Su-C4-QVCehW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profiles[[\"following\" , \"followers\" , \"n_posts\" ]].describe()"
      ],
      "metadata": {
        "id": "wnvtxA44CiZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts[[\"numbr_likes\",\"number_comments\"]].describe()"
      ],
      "metadata": {
        "id": "rrdwvnsXCigt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do a better analysis, we clean up our datasets as much as possible by following the steps described below.\n",
        "\n",
        "\n",
        "1.   **Dealing with duplicates rows**:\n",
        "      we drop every duplicates rows because contains useless informations \n",
        "2.    **Dealing with missing values**:\n",
        "      we check the percentage of missing values in each column, we drop a column that contains missing values over a set threshold. After we select some columns of interest and drop the NA\n",
        "3. **Dealing with outliers:** It is risky to include outliers in data-driven models. For some variables of interest, we have incorrect typing data or false profiles that influence the distribution so much, we want to detect them and decide whether to delete them or not.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s1r22STZeOiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Duplicates rows\n",
        "profiles.drop_duplicates(inplace=True)\n",
        "posts.drop_duplicates(inplace=True)\n",
        "locations.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "zLHqdQALQsvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dealing with missing values, columns that contains so much Na\n",
        "#check for null values / % of null values we have\n",
        "print('Null values percentage for locations dataset')\n",
        "print(locations.isnull().sum()/locations.shape[0])\n",
        "print('\\n')\n",
        "print('Null values percentage for profiles dataset')\n",
        "print(profiles.isnull().sum()/profiles.shape[0])\n",
        "print('\\n')\n",
        "print('Null values percentage for posts dataset')\n",
        "print(posts.isnull().sum()/posts.shape[0])"
      ],
      "metadata": {
        "id": "fuG5DEzWYUyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ee2e74-98e3-4f2d-8986-22156a0d0be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null values percentage for locations dataset\n",
            "sid                       0.000000\n",
            "id                        0.000000\n",
            "name                      0.000000\n",
            "street                    0.300153\n",
            "zip                       0.300275\n",
            "city                      0.083598\n",
            "region                    0.998279\n",
            "cd                        0.081795\n",
            "phone                     0.411643\n",
            "aj_exact_city_match       0.021657\n",
            "aj_exact_country_match    0.021657\n",
            "blurb                     0.602306\n",
            "dir_city_id               0.515285\n",
            "dir_city_name             0.515285\n",
            "dir_city_slug             0.515751\n",
            "dir_country_id            0.515353\n",
            "dir_country_name          0.515285\n",
            "lat                       0.006026\n",
            "lng                       0.006026\n",
            "primary_alias_on_fb       0.583897\n",
            "slug                      0.079196\n",
            "website                   0.390547\n",
            "cts                       0.000000\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "Null values percentage for profiles dataset\n",
            "sid                    0.000000\n",
            "profile_id             0.007195\n",
            "profile_name           0.000000\n",
            "firstname_lastname     0.063967\n",
            "description            0.455917\n",
            "following              0.234349\n",
            "followers              0.234349\n",
            "n_posts                0.234349\n",
            "url                    0.807017\n",
            "cts                    0.097235\n",
            "is_business_account    0.236000\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "Null values percentage for posts dataset\n",
            "sid                0.00000\n",
            "sid_profile        0.00000\n",
            "post_id            0.00000\n",
            "profile_id         0.00000\n",
            "location_id        0.00000\n",
            "cts                0.00000\n",
            "post_type          0.00000\n",
            "description        0.08095\n",
            "numbr_likes        0.00000\n",
            "number_comments    0.00000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove null colums over the threshold\n",
        "profiles.dropna(thresh = len(profiles)*.6, axis=1)\n",
        "locations.dropna(thresh = len(locations)*.6, axis=1)"
      ],
      "metadata": {
        "id": "kA_1PN-EZF6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to manage Na and outliers values to do a better analysis.\n",
        "For Na values, we have more than one option to do.\n",
        "\n",
        "**Replace with the mean:** if there are a lot of missing values we are concentrating the distribution in the mean value and we lose information about the true shape of the distributions.\n",
        "\n",
        "**Drop Na:** We are losing a relevant percentage of the information we have, however having a huge quantity of data, doesn't influence so much the distribution we are analyzing. So in this case we will go through this procedure ."
      ],
      "metadata": {
        "id": "6Y1Je5cqvS25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We show only some example of variable we are cleaning\n",
        "posts.dropna(subset=['numbr_likes', 'number_comments'])"
      ],
      "metadata": {
        "id": "ObgYqhgSnjsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The existence of misleading values has the potential to change the conclusion implied by the model. It is, therefore, important to detect and then decide whether to remove it or not from the dataset. Sometimes the data point may be extremely high or low but that does not mean it is an outlier that we want to get rid of. In the case of Instagram, for instance, a huge amount of followers' value is no sign of errors but can correspond to influencer accounts. On the opposite, an account with zero posts or zero followers usually corresponds to a fake or new account that we don't want to consider in our analysis (we will clean them up when we will analyze them in the next questions) .\n",
        "\n"
      ],
      "metadata": {
        "id": "_I6_5VsCnqx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_bins = 20\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "# Logaritmic histogram of the data and box plot\n",
        "plt.subplot(121)\n",
        "plt.hist(posts['numbr_likes'] ,  num_bins, facecolor='lightblue', alpha=0.5,log=True)\n",
        "plt.xlabel(\"log-value of number of likes\")\n",
        "plt.ylabel(\"density\")\n",
        "plt.title(\"Log distribution of \\n number of likes per account\", size = 16)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.hist(posts['number_comments'] ,  num_bins, facecolor='green', alpha=0.5,log=True)\n",
        "plt.xlabel(\"log-value of number of comments\")\n",
        "plt.ylabel(\"density\")\n",
        "plt.title(\"Log distribution of \\n number of comments per account\", size = 16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I-4SeA6df_on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "plt.boxplot(posts['numbr_likes'] )\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"number of likes\")\n",
        "plt.title(\"Box-Plot of number of likes\", size = 16)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.boxplot(posts['number_comments'])\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"number of comments\")\n",
        "plt.title(\"Box-Plot of number of comments\", size = 16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BVUCZ26lGm6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a logarithmic scale because the data covers a wide range of values and there is an evident skewness toward small values.\n",
        "\n",
        "Since most of the observations are concentrating to low value, we can show the actual distribution untill the 90% percentile."
      ],
      "metadata": {
        "id": "TzJuMWukx4nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "likes_outliers= posts[(posts.numbr_likes) < profiles.numbr_likes.quantile(0.95)]\n",
        "comments_outliers= posts[(profiles.numbr_likes) < profiles.numbr_likes.quantile(0.95)]"
      ],
      "metadata": {
        "id": "vIoGCQRlpl6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "sns.distplot(likes_outliers, hist=True, kde=True, \n",
        "             bins=int(180/5), color = 'darkblue', \n",
        "             hist_kws={'edgecolor':'black'},\n",
        "             kde_kws={'linewidth': 4}).set(title='Distribution of \\n number of likes per post')\n",
        "\n",
        "sns.distplot(comments_outliers, hist=True, kde=True, \n",
        "             bins=int(180/5), color = 'darkblue', \n",
        "             hist_kws={'edgecolor':'black'},\n",
        "             kde_kws={'linewidth': 4}).set(title='Distribution of \\n number of comments per post')"
      ],
      "metadata": {
        "id": "4gfYBbHU1RMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(121)\n",
        "plt.boxplot(likes_outliers )\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"number of likes\")\n",
        "plt.title(\"Box-Plot of number of likes\", size = 16)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.boxplot(comments_outlier)\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"number of comments\")\n",
        "plt.title(\"Box-Plot of number of comments\", size = 16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7lhen9ZipQ4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RQ7"
      ],
      "metadata": {
        "id": "XOcPDpLMvKi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 42710197\n",
        "s = 1000\n",
        "filename = \"/content/drive/MyDrive/ADM_HW2/instagram_posts.csv\"\n",
        "df2= profiles[['followers', 'sid']]\n",
        "l=[]\n",
        "for i in range(1000):\n",
        "  skip = sorted(random.sample(range(n),n-s))\n",
        "  df = pd.read_csv(filename, usecols= ['numbr_likes' , 'sid_profile'], skiprows=skip, sep='\\t')\n",
        "  df1=posts[['numbr_likes' , 'sid_profile']]\n",
        "  df1=df1.dropna()\n",
        "  df2=df2.dropna()\n",
        "  data=pd.merge(df1, df2, left_on=\"sid_profile\", right_on=\"sid\").drop('sid', axis=1)\n",
        "  l.append(sum(data.apply(lambda x :  x.numbr_likes / x.followers, axis = 1 ) > 0.20)/ data.shape[0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "6pw3op_MxhoM",
        "outputId": "422d489d-233a-4835-d02e-3f71953376bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-841bc8209d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'numbr_likes'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'sid_profile'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'numbr_likes'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'sid_profile'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             ):\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_usecols_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             raise ValueError(\n\u001b[0;32m--> 867\u001b[0;31m                 \u001b[0;34mf\"Usecols do not match columns, columns expected but not found: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m                 \u001b[0;34mf\"{missing}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['sid_profile', 'numbr_likes']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(filename, sep='\\t', usecols= ['numbr_likes' , 'sid_profile'],)"
      ],
      "metadata": {
        "id": "eVlJiq8j-vCG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}